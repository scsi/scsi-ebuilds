命令行CURL教程
2008/11/10

CURL? 嗯，說來話長了~~~~
這東西現在已經是蘋果機上內置的命令行工具之一了，可見其魅力之一斑

1) 二話不說，先從這裡開始吧！

curl http://www.yahoo.com

回車之後，www.yahoo.com 的html就稀里嘩啦地顯示在屏幕上了~~~~~

2) 嗯，要想把讀過來頁面存下來，是不是要這樣呢？

curl http://www.yahoo.com > page.html

當然可以，但不用這麼麻煩的！
用curl的內置option就好，存下http的結果，用這個option: -o

curl -o page.html http://www.yahoo.com

這樣，你就可以看到屏幕上出現一個下載頁面進度指示。等進展到100%，自然就OK咯

3) 什麼什麼？！訪問不到？肯定是你的proxy沒有設定了。
使用curl的時候，用這個option可以指定http訪問所使用的proxy服務器及其端口： -x

curl -x 123.45.67.89:1080 -o page.html http://www.yahoo.com

4) 訪問有些網站的時候比較討厭，他使用cookie來記錄session信息。
像IE/NN這樣的瀏覽器，當然可以輕易處理cookie信息，但我們的curl呢？.....
我們來學習這個option: -D <--
這個是把http的response裡面的cookie信息存到一個特別的文件中去

curl -x 123.45.67.89:1080 -o page.html -D cookie0001.txt http://www.yahoo.com

這樣，當頁面被存到page.html的同時，cookie信息也被存到了cookie0001.txt裡面了

5）那麼，下一次訪問的時候，如何繼續使用上次留下的cookie信息呢？要知道，很多網站都是靠監視你的cookie信息， 來判斷你是不是不按規矩訪問他們的網站的。
這次我們使用這個option來把上次的cookie信息追加到http request裡面去： -b

curl -x 123.45.67.89:1080 -o page1.html -D cookie0002.txt -b cookie0001.txt http://www.yahoo.com

這樣，我們就可以幾乎模擬所有的IE操作，去訪問網頁了！

6） 稍微等等~~~~~我好像忘記什麼了~~~~~

對了！是瀏覽器信息~~~~
有些討厭的網站總要我們使用某些特定的瀏覽器去訪問他們，有時候更過分的是，還要使用某些特定的版本~~~~
NND，哪裡有時間為了它去找這些怪異的瀏覽器呢！？
好在curl給我們提供了一個有用的option，可以讓我們隨意指定自己這次訪問所宣稱的自己的瀏覽器信息：-A

curl -A "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)" -x 123.45.67.89:1080 -o page.html -D cookie0001.txt http://www.yahoo.com

這樣，服務器端接到訪問的要求，會認為你是一個運行在Windows
2000上的IE6.0，嘿嘿嘿，其實也許你用的是蘋果機呢！

而"Mozilla/4.73 [en] (X11; U; Linux 2.2; 15
i686"則可以告訴對方你是一台PC上跑著的Linux，用的是Netscape 4.73，呵呵呵

7） 另外一個服務器端常用的限制方法，就是檢查http訪問的referer。比如你先訪問首頁，再訪問裡面所指定的下載頁，這第二次訪問的referer地址就是第一次訪問成功後的頁面地址。這樣，服務器端只要發現對下載頁面某次訪問的referer地址不 是首頁的地址，就可以斷定那是個盜連了~~~~~
討厭討厭~~~我就是要盜連~~~~~！！
幸好curl給我們提供了設定referer的option： -e

curl -A "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)" -x 123.45.67.89:1080 -e "mail.yahoo.com" -o page.html -D cookie0001.txt http://www.yahoo.com

這樣，就可以騙對方的服務器，你是從mail.yahoo.com點擊某個鏈接過來的了，呵呵呵

8）寫著寫著發現漏掉什麼重要的東西了！----- 利用curl 下載文件
剛才講過了，下載頁面到一個文件裡，可以使用 -o ，下載文件也是一樣。
比如，

curl -o 1.jpg http://cgi2.tky.3web.ne.jp/~zzh/screen1.JPG

這裡教大家一個新的option： -O
大寫的O，這麼用：

curl -O http://cgi2.tky.3web.ne.jp/~zzh/screen1.JPG

這樣，就可以按照服務器上的文件名，自動存在本地了！
再來一個更好用的。
如果screen1.JPG以外還有screen2.JPG、screen3.JPG、....、screen10.JPG需要下載，難不成還要讓我們寫一個script來完成這些操作？
不干！
在curl裡面，這麼寫就可以了：

curl -O http://cgi2.tky.3web.ne.jp/~zzh/screen[1-10].JPG

呵呵呵，厲害吧？！~~~

9） 再來，我們繼續講解下載！

curl -O http://cgi2.tky.3web.ne.jp/~{zzh,nick}/[001-201].JPG

這樣產生的下載，就是
~zzh/001.JPG
~zzh/002.JPG
...
~zzh/201.JPG
~nick/001.JPG
~nick/002.JPG
...
~nick/201.JPG
夠方便的了吧？哈哈哈
咦？高興得太早了。
由於zzh/nick下的文件名都是001，002...，201，下載下來的文件重名，後面的把前面的文件都給覆蓋掉了~~~
沒關係，我們還有更狠的！

curl -o #2_#1.jpg http://cgi2.tky.3web.ne.jp/~{zzh,nick}/[001-201].JPG

--這是.....自定義文件名的下載？
--對頭，呵呵！
#1是變量，指的是{zzh,nick}這部分，第一次取值zzh，第二次取值nick
#2代表的變量，則是第二段可變部分---[001-201]，取值從001逐一加到201
這樣，自定義出來下載下來的文件名，就變成了這樣：
原來： ~zzh/001.JPG ---> 下載後： 001-zzh.JPG
原來： ~nick/001.JPG ---> 下載後： 001-nick.JPG
這樣一來，就不怕文件重名啦，呵呵

9） 繼續講下載
我們平時在windows平台上，flashget這樣的工具可以幫我們分塊並行下載，還可以斷線續傳。
curl在這些方面也不輸給誰，嘿嘿
比如我們下載screen1.JPG中，突然掉線了，我們就可以這樣開始續傳

curl -c -O http://cgi2.tky.3wb.ne.jp/~zzh/screen1.JPG

當然，你不要拿個flashget下載了一半的文件來糊弄我~~~~別的下載軟件的半截文件可不一定能用哦~~~
分塊下載，我們使用這個option就可以了： -r
舉例說明
比如我們有一個http://cgi2.tky.3web.ne.jp/~zzh/zhao1.mp3
要下載（趙老師的電話朗誦 :D ）
我們就可以用這樣的命令：

curl -r 0-10240 -o "zhao.part1" http:/cgi2.tky.3web.ne.jp/~zzh/zhao1.mp3 &

curl -r 10241-20480 -o "zhao.part1" http:/cgi2.tky.3web.ne.jp/~zzh/zhao1.mp3 &

curl -r 20481-40960 -o "zhao.part1" http:/cgi2.tky.3web.ne.jp/~zzh/zhao1.mp3 &

curl -r 40961- -o "zhao.part1" http:/cgi2.tky.3web.ne.jp/~zzh/zhao1.mp3

這樣就可以分塊下載啦。
不過你需要自己把這些破碎的文件合併起來
如果你用UNIX或蘋果，用 cat zhao.part* > zhao.mp3就可以
如果用的是Windows，用copy /b 來解決吧，呵呵
上面講的都是http協議的下載，其實ftp也一樣可以用。
用法嘛，

curl -u name:passwd ftp://ip:port/path/file

或者大家熟悉的

curl ftp://name:passwd@ip:port/path/file


10) 說完了下載，接下來自然該講上傳咯
上傳的option是 -T
比如我們向ftp傳一個文件：

curl -T localfile -u name:passwd ftp://upload_site:port/path/

當然，向http服務器上傳文件也可以

比如

curl -T localfile http://cgi2.tky.3web.ne.jp/~zzh/abc.cgi

注意，這時候，使用的協議是HTTP的PUT method
剛才說到PUT，嘿嘿，自然讓老服想起來了其他幾種methos還沒講呢！
GET和POST都不能忘哦。
http提交一個表單，比較常用的是POST模式和GET模式
GET模式什麼option都不用，只需要把變量寫在url裡面就可以了
比如：

curl http://www.yahoo.com/login.cgi?user=nickwolfe&password=12345

而POST模式的option則是 -d

curl -d "user=nickwolfe&password=12345" http://www.yahoo.com/login.cgi

就相當於向這個站點發出一次登陸申請~~~~~
到底該用GET模式還是POST模式，要看對面服務器的程序設定。
一點需要注意的是，POST模式下的文件上的文件上傳，比如

<form method="POST" enctype="multipar/form-data" action="http://cgi2.tky.3web.ne.jp/~zzh/up_file.cgi">
<input type=file name=upload>
<input type=submit name=nick value="go">
</form>

這樣一個HTTP表單，我們要用curl進行模擬，就該是這樣的語法：

curl -F upload=@localfile -F nick=go http://cgi2.tky.3web.ne.jp/~zzh/up_file.cgi

羅羅嗦嗦講了這麼多，其實curl還有很多很多技巧和用法
比如 https的時候使用本地證書，就可以這樣

curl -E localcert.pem https://remote_server

再比如，你還可以用curl通過dict協議去查字典~~~~~

curl dict://dict.org/d:computer

今天就先講到這裡吧，呵呵。瘋狂的curl功能，需要你---一起來發掘。


CMD與Curl雙劍合璧：自動合併多頁主題
現實需求
當遇到長串經典的討論帖；
當看到分多頁的軟件教程；
當發現讓人愛不釋手的連載小說的時候。
如何儲存這些有很多分頁的內容就成為了一件冗雜而又枯燥的機械勞動。
無論是手工複製還是依靠軟件儲存，都需要大量的人為干預，這是身為智慧生物的我們所不能容忍的。
既然電腦的出現就是替代人進行一些繁複的工作的，那為什麼不把盡可能多的工作扔給它們呢？
可惜豆腐目前還沒有發現一款軟件可以滿足我的要求，既然沒有現成的可用那就自己動手吧。
思路分析
要解決一個問題必須先有一個環境，畢竟一個方案不可能通吃所有問題。我們就先設問題是要合併論壇中常見的多頁主題。
要合併一個多頁主題，我們首先得獲取這個主題的每一個分頁的內容，這種重複性的工作讓機器來做是再適合不過的了。
其次我們需要分辨用戶貼出的內容從哪裡開始，在哪裡結束。這部分第一次需要人來完成，後面的就交給機器吧。
最後我們需要獲取我們需要的內容並把它重新組織起來產生最終的成果，這同樣只需機器就可以很好的完成。
只要我們滿足了上面三點，我們就可以把自己從重複勞動中解救出來做其它的事情了。
解決方案
由於高階語言需要專門的學習和配套的軟件，這無形提高了應用的難度，最終豆腐選擇了用CMD命令行來完成這個工作。
當然，CMD命令中是沒有獲取網頁內容的功能的，我們還需要Curl這個強大的命令行工具來助我們一臂之力。
我們就以合併CCF精品技術論壇的MPlayer 2006-03-03 K&K 更新在 992
樓為例，順著剛才的思路來一步步嘗試以達到最終的Goal。
網頁抓取
在Curl的幫助下，我們可以輕鬆的通過命令行來抓取我們想要的網頁：
CODE:
curl -o tmp1.txt http://bbs..net/bbs/showthread.php?t...9&page=1&pp=15
[Copy to clipboard]
這樣我們就把該主題第一頁的內容儲存在了tmp1.txt文件中。
對於某些需要檢測瀏覽器訊息的網站，我們可以用
CODE:
-A "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)"
[Copy to clipboard]來偽裝成IE瀏覽器。
對於需要使用cookies的網站，我們可以用
CODE:
-D cookie1.txt
[Copy to clipboard]來儲存cookies，用
CODE:
-b cookie1.txt
[Copy to clipboard]來讀取cookies。
對於防盜鏈的網站，我們可以用
CODE:
-e "http://bbs..net/"
[Copy to clipboard]
來偽裝成從某個相關聯接進入的。 再與CMD中強大的 FOR
命令和變數相結合，加上人類的小小智慧，就可以打造出自動抓取該主題的全部內容的腳本。
分析該主題的URL，我們可以知道 page=
表示頁數，這為自動化處理提供了基礎，同時我們知道該主題有73頁，最終的抓取腳本如下：
CODE:
@echo off
setlocal ENABLEDELAYEDEXPANSION
set last=1
for /l %%i in (1,1,73) do (
echo %%i
curl -b cookie!last!.txt -D cookie%%i.txt -A "Mozilla/4.0 (compatible; MSIE
6.0; Windows NT 5.0)" -e
"http://bbs.et8.net/bbs/showthread.php?t=634659^&page=!last!^&pp=15" -o
tmp%%i.txt http://bbs.et8.net/bbs/showthread.php?t=634659^&page=%%i^&pp=15
set /a last=%%i-1
)
copy tmp*.txt temp.txt
del cookie*.txt
del tmp*.txt
endlocal
[Copy to clipboard]
將上面腳本儲存為 grab.cmd 執行後我們就的到了儲存了該主題全部73頁內容的
temp.txt 文件。
內容分析
由於CMD字元處理的問題，我們先把 temp.txt 另存為 ANSI 編碼。
分析單頁的內容後豆腐發現該論壇程式在用戶內容開始之前有一個每頁唯一的 ，
而在結束的時候有一個同樣唯一的 ，這正是我們所希望找到的可以作為標誌位的地方。
文本處理
由於 FOR 命令一次只能以同樣的規則處理一行的內容，於是豆腐便採用 FOR
嵌套的方式來處理整個大文件。
先用
CODE:
for /f "delims=" %%i in (temp.txt) do ( echo %%i >tmp.txt )
[Copy to clipboard]將 temp.txt 的內容一次一行地寫入 tmp.txt。
再套用另一個 FOR 來處理 tmp.txt 的一行。
標誌設置
我們可以通過 FOR 的 delims= 和 tokens= 參數來分割和儲存一行的內容
我們用
CODE:
for /f "tokens=1-3 delims== " %%j in (tmp.txt)
[Copy to clipboard]參數設定以 ""、"-"、"="、" "來分割一行，
並把分割後的前三段內容存入 %%j %%k %%l 三個變數中。接著我們用 if
語句來判斷這三個變數是否符合設置標誌位的條件：
CODE:
if "%%j"=="div" if "%%k"=="id" if %%l=="posts" set flag=1
if "%%j"=="start" if "%%k"=="content" if "%%l"=="table" set flag=0
[Copy to clipboard]
flag=1 代表用戶內容開始，flag=0代表用戶內容結束。
內容剪裁
由於CMD命令行處理的限制，HTML中的註釋開始符號 "CODE:
for /f "tokens=1-8 delims=" if not "%%s"=="-->" if not "%%r"=="-->" if not
"%%q"=="-->" if not "%%p"=="-->" if not "%%o"=="-->" if not "%%m"=="ECHO" if
!flag!==1 echo %%i >>new.htm)
[Copy to clipboard]
同時，我們也完成了把開始標誌位後的內容存入 new.htm 的工作。
最終腳本
CODE:
@echo off
setlocal ENABLEDELAYEDEXPANSION
set flag=0
for /f "delims=" %%i in (temp.txt) do (
echo %%i >tmp.txt
for /f "tokens=1-3 delims== " %%j in (tmp.txt) do (
if "%%j"=="div" if "%%k"=="id" if %%l=="posts" set flag=1
if "%%j"=="start" if "%%k"=="content" if "%%l"=="table" set flag=0
for /f "tokens=1-8 delims=" if not "%%s"=="-->" if not "%%r"=="-->" if not
"%%q"=="-->" if not "%%p"=="-->" if not "%%o"=="-->" if not "%%m"=="ECHO" if
!flag!==1 echo %%i >>new.htm
)
)
)
del tmp.txt
endlocal
[Copy to clipboard]
儲存腳本為 merge.cmd 執行後得到合併出的 new.htm
文件就是該主題全部1083帖的內容。
優化改進
該腳本只完成了抓取文本內容的工作，我們還可以通過判斷 IMG 元素來找到圖片內容，
並把 src 內容後面的路徑補完成完整路徑，就可以正確顯示出內容中的圖片。
後記總結
CMD和Curl相結合可以完成很多批量的複雜工作，雖然第一次多花點時間，但之後就可以方便的使用了。
該腳本可以順利抓取合併CCF技術論壇的任意主題以及部分基於vBulletin的論壇，但對於其它論壇還需要分別修改才可使用。
本文為chenke_ikari原創，首發於豆腐的簡陋小屋
本文采用Creative Commons 署名-非商業性使用-相同方式共享 2.5 China 許可協議
進行許可
